import streamlit as st
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- ArayÃ¼z BaÅŸlÄ±klarÄ± ---
st.set_page_config(page_title="Motivasyon Chatbotu (RAG)", layout="wide")
st.title("Akbank GenAI Bootcamp: Motivasyon Chatbotu ğŸ’¡")
st.markdown("Merhaba! Ben, yÃ¼klenen motivasyonel metinlere dayalÄ± sorularÄ±nÄ±zÄ± yanÄ±tlamak iÃ§in tasarlanmÄ±ÅŸ bir RAG (Retrieval Augmented Generation) modeliyim. LÃ¼tfen sadece metinlerin iÃ§eriÄŸiyle ilgili sorular sorun.")


# --- RAG Pipeline TanÄ±mlama Fonksiyonu ---
@st.cache_resource
def setup_rag_pipeline():
    """
    RAG pipeline'Ä±nÄ± kurar ve VektÃ¶r VeritabanÄ±nÄ± hazÄ±rlar.
    Bu fonksiyon, uygulamayÄ± her baÅŸlattÄ±ÄŸÄ±nÄ±zda yalnÄ±zca bir kez Ã§alÄ±ÅŸÄ±r.
    """
    # 1. Veri YÃ¼kleme (DosyalarÄ± Streamlit ortamÄ±nda varsayÄ±yoruz)
    # NOT: Veri setini (motivasyon1-3.txt) GitHub'a yÃ¼klememiz gerekecek.
    
    # Hata: Colab'da yÃ¼klenen dosyalarÄ± buradan okuyamayÄ±z. 
    # GeÃ§ici olarak metinleri doÄŸrudan buraya ekleyelim, 
    # aksi takdirde deploy zorlaÅŸÄ±r. (Normalde dosyalarÄ± yÃ¼klememiz gerekir.)
    
    motivasyonel_metin = """
    Kendine inan, Ã§Ã¼nkÃ¼ sen dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼nden daha gÃ¼Ã§lÃ¼sÃ¼n. BugÃ¼n kÃ¶tÃ¼ olabilir ama yarÄ±n yeni bir baÅŸlangÄ±Ã§. KÃ¼Ã§Ã¼k adÄ±mlar bÃ¼yÃ¼k farklar yaratÄ±r.
    VazgeÃ§mek kolaydÄ±r ama devam etmek kazandÄ±rÄ±r. Her baÅŸarÄ±, bir hayalle baÅŸlar. Bir nefes bile umut taÅŸÄ±r.
    ÃœzÃ¼ntÃ¼ye de izin ver, hissetmek gÃ¼Ã§lendirir. MutluluÄŸu bulacak olan, sensin. Ä°yileÅŸmek iÃ§in kÃ¶tÃ¼ anÄ±larÄ± bavul gibi taÅŸÄ±mayÄ± bÄ±rakmalÄ±sÄ±n.
    """
    
    # TextLoader yerine doÄŸrudan metin iÃ§eriÄŸini kullanalÄ±m
    from langchain.schema import Document
    documents = [Document(page_content=motivasyonel_metin, metadata={"source": "motivasyon_metinleri"})]
    
    # 2. ParÃ§alama
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(documents)

    # 3. Embedding ve VektÃ¶r VeritabanÄ± OluÅŸturma
    # API key'in Streamlit Secrets veya OS Environment'tan geldiÄŸini varsayÄ±yoruz.
    if "GEMINI_API_KEY" not in os.environ:
        st.error("LÃ¼tfen Gemini API AnahtarÄ±nÄ±zÄ± Streamlit'e ekleyin.")
        st.stop()
        
    embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)
    
    # 4. Retriever ve Model TanÄ±mlama
    retriever = vectorstore.as_retriever()
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    
    # 5. Prompt Åablonu
    template = """
    Sen bir motivasyon chatbotusun. Sadece verilen baÄŸlamÄ± (context) kullanarak, soruyu yanÄ±tla. 
    EÄŸer baÄŸlamda soruyla ilgili bilgi bulamÄ±yorsan, kibarca "Bu bilgi elimdeki motivasyon metinlerinde bulunmamaktadÄ±r." diye cevap ver.
    CevaplarÄ± TÃ¼rkÃ§e ve motive edici bir dille ver.

    BaÄŸlam: {context} 
    Soru: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    
    # 6. RAG Zinciri (Chain)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# RAG pipeline'Ä± kur
rag_chain = setup_rag_pipeline()


# --- Chatbot ArayÃ¼zÃ¼ MantÄ±ÄŸÄ± ---

# KonuÅŸma geÃ§miÅŸini baÅŸlat
if "messages" not in st.session_state:
    st.session_state.messages = []

# GeÃ§miÅŸ mesajlarÄ± gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan input al
if prompt := st.chat_input("Motivasyonel metinlerle ilgili sorunuz nedir?"):
    # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle ve gÃ¶ster
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # LLM yanÄ±tÄ±nÄ± al ve gÃ¶ster
    with st.chat_message("assistant"):
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
            # RAG zincirini Ã§alÄ±ÅŸtÄ±r
            response = rag_chain.invoke(prompt)
            st.markdown(response)
            
    # Asistan yanÄ±tÄ±nÄ± geÃ§miÅŸe ekle
    st.session_state.messages.append({"role": "assistant", "content": response})
